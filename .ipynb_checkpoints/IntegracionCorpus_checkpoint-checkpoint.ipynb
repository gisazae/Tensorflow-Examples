{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SmartSec4COP \n",
    "F. Mu√±oz, G. Isaza\n",
    "Remember the pipeline\n",
    "ClasificadorCNN.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gisazae/Tensorflow-Examples/blob/master/IntegracionCorpus_checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wir0VexSaWrh"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ig_MK_c-t0wa"
   },
   "source": [
    "### Division de grupos de texto\n",
    "\n",
    "#### Dividir en:\n",
    "\n",
    "- solo texto de atacantes.\n",
    "- todo el conjunto de chats.\n",
    "- grupos de conversaciones por linea en 2 archivos, un archivo de atacantes y otro de otros msjs.\n",
    "- grupos de conversaciones por linea en 2 archivos, un archivo de atacantes con victimas y otro de otros msjs.\n",
    "\n",
    "\n",
    "- Los anteriores deben tener un estudio de longitud para ajustar la NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUSPZlSKt0we"
   },
   "source": [
    "### word to vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "vVxYnqe8t0wh"
   },
   "source": [
    "Se puede utilizar, pero hay que evaluar:\n",
    "\n",
    "\n",
    "from tensorflow.models.embedding import word2vec as w2v\n",
    "\n",
    "- Hacer graficas de longitud por cantidad de palabras por mensaje y por conversacion.\n",
    "\n",
    "- Para aumentar la precision se deben dividir las longitudes de conversaciones, y de cada mensaje para los batch, de esa forma la presicion deberia aumentar ya que el contexto se mantendria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "na3vmrC0t0wl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10.0, 7.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aw85lF-It0wv"
   },
   "source": [
    "#### Nota:\n",
    "- Al realizar una revision de los tokens mas frecuentes y menos frecuentes se encintra que hay algunos tokens conformados por emoticones, se va a utilizar un tokenizador casual de NLTK el cual esta fabricado para textos tipo Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B12pWCbSt0wy"
   },
   "outputs": [],
   "source": [
    "string=\"b8810fee2f4a71f849f3f7409546d1d9 : see also http://dev.opera.com/articles/view/evolving-the-internet-on-your-phone-des-1/ which does\"\n",
    "\n",
    "string=re.sub(r\"\\S{15,}\\:\\s\",\"\",string)\n",
    "string=re.sub(r\"\\S{15,}\\s\\:\\s\",\"\",string)\n",
    "string=re.sub(r\"\\S{15,}\\,\\s\",\"\",string)\n",
    "string=re.sub(r\"\\S{15,}\\s\\,\\s\",\"\",string)\n",
    "string=re.sub(r\"\\S{15,}\\-\\s\",\"\",string)\n",
    "string=re.sub(r\"\\S{15,}\\s\\-\\s\",\"\",string)\n",
    "\n",
    "print(string)\n",
    "\n",
    "# : , -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lkQ_kHTJt0xE"
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\.\", \" \\. \", string)\n",
    "    #string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    #string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    #string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    #string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "\n",
    "    #print(type(string))\n",
    "    #string=string.split()\n",
    "    return string.strip().lower()\n",
    "\n",
    "def clean_chats(string):\n",
    "    string=re.sub(r\"\\S{15,}\\:\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\s\\:\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\,\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\s\\,\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\-\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\s\\-\\s\",\"\",string)\n",
    "    return string\n",
    "def dummy(string):\n",
    "    string=clean_chats(string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCPUELcpt0xL"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "longLines=[]\n",
    "lines = open('PAN12/full-otros-y-ataques.txt').readlines()\n",
    "longLines=[w for w in lines if len(w)>500]\n",
    "lines=[w for w in lines if len(w)<500]\n",
    "print(len(longLines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzZu66Hmt0xQ"
   },
   "outputs": [],
   "source": [
    "print(longLines[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2F2jwyWt0xU"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "numthreads = 12\n",
    "numlines = 1000\n",
    "result_list=[]\n",
    "\n",
    "pool = multiprocessing.Pool(processes=numthreads)\n",
    "result_list = pool.map_async(dummy, lines, numlines)\n",
    "print(len(result_list.get()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5m0idqRwt0xY"
   },
   "outputs": [],
   "source": [
    "f=open(\"delete.txt\",\"w\")\n",
    "for item in result_list.get():\n",
    "    for i in item:\n",
    "        f.write(i)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QV4_Fvrjt0xc"
   },
   "outputs": [],
   "source": [
    "!wc -l PAN12/full-otros-y-ataques.txt\n",
    "!diff delete.txt PAN12/full-otros-y-ataques.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i1ZsQp3Wt0xf"
   },
   "outputs": [],
   "source": [
    "#def read_data(f):\n",
    "#    txt=f.read()\n",
    "#    txt=clean_chats(txt)\n",
    "#    print(\"\\nLen at read %s \\n\" % len(txt))\n",
    "#    txtaux=clean_str(txt)\n",
    "#    data = tf.compat.as_str(txtaux).split()\n",
    "#    return data,txt\n",
    "\n",
    "####################\n",
    "############\n",
    "### Definicion de archivo  \n",
    "############\n",
    "####################\n",
    "\n",
    "#f=open(\"PAN12/full-otros-y-ataques.txt\",\"r\")\n",
    "f=open(\"delete.txt\",\"r\")\n",
    "#wordsSimple,txt = read_data(f)\n",
    "txt=f.read()\n",
    "#txt=clean_chats(txt)\n",
    "f.close()\n",
    "\n",
    "#print('\\nData size simple: %s \\n' % len(wordsSimple))\n",
    "\n",
    "#wordsSimpleSet=set(wordsSimple)\n",
    "#print(\"\\nVocabulary %s \\n\" % len(wordsSimpleSet))\n",
    "#vocabulary_size =len(wordsSimpleSet)\n",
    "\n",
    "\n",
    "#fd=nltk.FreqDist(wordsSimple)\n",
    "\n",
    "#fd.plot(50, cumulative=False)\n",
    "\n",
    "#hap=fd.hapaxes()\n",
    "#print(\"\\nNum words less common %s \\n\" % len(hap))\n",
    "#print(hap[:50])\n",
    "\n",
    "\n",
    "print(\"\\n\\nLen at read %s \\n\" % len(txt))\n",
    "\n",
    "twtTks=nltk.tokenize.TweetTokenizer(preserve_case=False,reduce_len=True)\n",
    "txtTks=twtTks.tokenize(txt)\n",
    "print('\\nData size Tweet Tokenizer %s \\n' % len(txtTks))\n",
    "\n",
    "txtTksSet=set(txtTks)\n",
    "print(\"\\nVocabulary %s \\n\" % len(txtTksSet))\n",
    "vocabulary_size =len(txtTksSet)\n",
    "\n",
    "txtTksFd=nltk.FreqDist(txtTks)\n",
    "\n",
    "txtTksFd.plot(50, cumulative=False)\n",
    "\n",
    "txtTksFdHap=txtTksFd.hapaxes()\n",
    "print(\"\\nNum words less common %s \\n\" % len(txtTksFdHap))\n",
    "print(txtTksFdHap[:50])\n",
    "\n",
    "print(\"# DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-gWwNa-vt0xk"
   },
   "source": [
    "### OJO:\n",
    "- Hay que ensayar con los 2 tipo de tokenizadores\n",
    "- y se ESTAN eliminando los Hapaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SV8lug3Jt0xm"
   },
   "outputs": [],
   "source": [
    "#vocabulary_size = len(wordsSet)\n",
    "#words=wordsSimple\n",
    "#vocabulary_size = len(txtTksSet)\n",
    "#words=txtTks\n",
    "#vocabulary_size = len(txtTksSet)-len(txtTksFdHap)+1\n",
    "words=txtTks\n",
    "\n",
    "tmpset=set(txtTksFdHap)\n",
    "print(\"data set: \",len(words))                \n",
    "modwords=[w if w not in tmpset else \"UNKVERSION2\" for w in words]\n",
    "print(\"con las palabras menos utilizadas modificadas:\", len(modwords))\n",
    "longWords=[w for w in modwords if len(w)>20]\n",
    "\n",
    "print(\"Cantidad de palabras extensas:\",len(longWords))\n",
    "longWordsSet=set(longWords)\n",
    "print(\"Diccionario de palabras extensas:\",len(longWordsSet))\n",
    "\n",
    "modwFd=nltk.FreqDist(modwords)\n",
    "modwFd.plot(50, cumulative=False)\n",
    "\n",
    "longWordsFd=nltk.FreqDist(longWords)\n",
    "longWordsFd.plot(50, cumulative=False)\n",
    "\n",
    "print(longWordsFd.hapaxes())\n",
    "\n",
    "shortWords=[w for w in modwords if len(w)<20]\n",
    "\n",
    "words=shortWords\n",
    "vocabulary_size=len(set(words))\n",
    "print(\"Vocab size\",vocabulary_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VgpKSfLXt0xp"
   },
   "outputs": [],
   "source": [
    "nltktxt=nltk.Text(txtTks)\n",
    "mc=longWordsFd.most_common(10)\n",
    "mc2=[x[0] for x in mc]\n",
    "nltktxt.dispersion_plot(mc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rJI5eee1t0xs"
   },
   "outputs": [],
   "source": [
    "nltktxt.concordance(mc2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5f9wpBl6t0xv"
   },
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "    \n",
    "    \n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "#del words  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "data_index = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6mV6Rbogt0xz"
   },
   "source": [
    "- Explicacion de los parametros:   http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wwp4Ds4-t0xz"
   },
   "outputs": [],
   "source": [
    "data_index=0\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=16, num_skips=2, skip_window=1)# 16 a 512, 8,2,1\n",
    "print(\"batch: \",len(batch))\n",
    "print(\"labels: \",len(labels))\n",
    "for i in range(16):\n",
    "    print(batch[i], reverse_dictionary[batch[i]],\n",
    "        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "tjkgslVwt0x3"
   },
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kl5qqow_t0x4"
   },
   "outputs": [],
   "source": [
    "#!rm -r embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rf82w_qNt0x7"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "batch_size = 128  # 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size],name=\"train_input\")\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1],name=\"train_labels\")\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32,name=\"valid_dataset\")\n",
    "\n",
    "    # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "    with tf.device('/cpu:0'):\n",
    "        # Look up embeddings for inputs.\n",
    "        embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name=\"embeddings\")\n",
    "        #variable_summaries(embeddings)\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs,name=\"lookup\")\n",
    "\n",
    "        # Construct the variables for the NCE loss\n",
    "        nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                stddev=1.0 / math.sqrt(embedding_size)),name=\"nce_Ws\")\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]),name=\"nce_Bs\")\n",
    "\n",
    "    # Compute the average NCE loss for the batch.\n",
    "    # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "    # time we evaluate the loss.\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                    b num_classes=vocabulary_size))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    \n",
    "    # For embedd visualization\n",
    "    #embedding_var = tf.Variable(normalized_embeddings[:500],name=\"embed_to_save\")\n",
    "\n",
    "    \n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opajH1Dft0x-"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_steps = 10001\n",
    "embed_viz_size = 10000\n",
    "data_index=0\n",
    "timestamp = str(int(time.time()))\n",
    "\n",
    "print(\"Lanzamiento No: \", timestamp)\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(\"embed/\" + timestamp + '/train',\n",
    "                                          session.graph)\n",
    "    test_writer = tf.summary.FileWriter(\"embed/\" + timestamp + '/test')\n",
    "    init.run()\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(\n",
    "            batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        summary,_, loss_val = session.run([merged,optimizer, loss], feed_dict=feed_dict)\n",
    "        train_writer.add_summary(summary, step)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print(\"Average loss at step \", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if (step % 10000 == 0) and (False):\n",
    "            #saver = tf.train.Saver()\n",
    "            #saver.save(session, os.path.join(\"embed\", \"model.ckpt\"), step)\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = \"Nearest to %s:\" % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    \n",
    "    # For embedd visualization\n",
    "    print(\"\\n#### Skip-gram end\")\n",
    "    # Link this tensor to its metadata file (e.g. labels).\n",
    "    tsvLabels = [re.sub(r\"\\n\", \"\",reverse_dictionary[i]).encode('utf8')+\"\\n\" for i in xrange(embed_viz_size)]\n",
    "#    tsvLabels = [re.sub(r\"\\n\", \"\",reverse_dictionary[i])+\"\\n\" for i in xrange(embed_viz_size)]\n",
    "    fLabels=open(\"embed/\"+timestamp+\"/train/metadata.tsv\",\"w\")\n",
    "    print(\"tsvLabels: \")\n",
    "    print(len(tsvLabels))\n",
    "    print(type(tsvLabels))\n",
    "    #print(tsvLabels)\n",
    "    fLabels.writelines(tsvLabels)\n",
    "    fLabels.close()\n",
    "    \n",
    "    #embedding_var = tf.Variable(normalized_embeddings[:500],name=\"embed_to_save\")\n",
    "    #embedding_var = tf.Variable(final_embeddings[:500],name=\"embed_to_save\")\n",
    "    embedding_var = tf.Variable(normalized_embeddings[:embed_viz_size], name='embedding_labels')\n",
    "    session.run(embedding_var.initializer)\n",
    "    embeddingTot = tf.Variable(normalized_embeddings, name='embeddingsTot')\n",
    "    session.run(embeddingTot.initializer)\n",
    "    # Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n",
    "    config = projector.ProjectorConfig()\n",
    "    # Use the same LOG_DIR where you stored your checkpoint.\n",
    "    summary_writer = tf.summary.FileWriter(\"embed/\"+timestamp+\"/train\")\n",
    "    # You can add multiple embeddings. Here we add only one.\n",
    "    embeddingConf = config.embeddings.add()\n",
    "    embeddingConf.tensor_name = embedding_var.name\n",
    "    embeddingConf.metadata_path = os.path.join(\"embed/\"+timestamp+\"/train/\", 'metadata.tsv')\n",
    "    # Saves a configuration file that TensorBoard will read during startup.\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "    # save checkpoint\n",
    "    saver = tf.train.Saver([embedding_var,embeddingTot])\n",
    "    saver.save(session, os.path.join(\"embed/\"+timestamp+\"/train/\", \"modelskip.ckpt\"), 1)\n",
    "    print(\"\\n#### embedds\")\n",
    "    \n",
    "    \n",
    "print(\"index: \",data_index)\n",
    "print(\"data len: \",len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "ciKH8Y9Ft0yB"
   },
   "source": [
    "# Integracion de embeds con Lexicon, diccionario Empath (U. Stanford) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxG7Opait0yC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import multiprocessing\n",
    "import itertools\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 7.0)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rOQdk_L0t0yE"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NLTK_TWEET_TOKENIZER = TweetTokenizer(preserve_case=False,reduce_len=True)\n",
    "\n",
    "def tokenizerFn(iterator):\n",
    "    for value in iterator:\n",
    "        try:\n",
    "            yield NLTK_TWEET_TOKENIZER.tokenize(value)\n",
    "        except TypeError:       # this is a hack to avoid the error\n",
    "            yield []     \n",
    "\n",
    "def clean_chats(string):\n",
    "    string=re.sub(r\"\\S{15,}\\:\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\s\\:\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\,\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\s\\,\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\-\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\s\\-\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{20,}\",\"\",string)\n",
    "    return string.strip()\n",
    "def dummy(string):\n",
    "    string=clean_chats(string)\n",
    "    return string\n",
    "\n",
    "\n",
    "#lines = open('PAN12/conversaciones-ataques.txt').readlines()\n",
    "#lines = open('PAN12/atacantes.txt').readlines()\n",
    "lines = open('PAN12/full-otros-y-ataques.txt').readlines()\n",
    "lines=[unicode(w,errors='ignore') for w in lines if len(w)<500]\n",
    "\n",
    "numthreads = 12\n",
    "numlines = 1000\n",
    "result_list=[]\n",
    "\n",
    "pool = multiprocessing.Pool(processes=numthreads)\n",
    "result_list = pool.map_async(dummy, lines, numlines)\n",
    "\n",
    "txt=result_list.get()\n",
    "#txt=\" \".join(result_list.get())\n",
    "#read con decode\n",
    "#para verlo bien .encode(\"unicode_escape\")\n",
    "# del var\n",
    "\n",
    "#############\n",
    "#############\n",
    "\n",
    "txtLen=50\n",
    "\n",
    "############\n",
    "############\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(txtLen,min_frequency=1,tokenizer_fn=tokenizerFn)\n",
    "\n",
    "txtTransform=list(vocab_processor.fit_transform(txt))\n",
    "\n",
    "print(\"Vocabulary:  %s\" % len(vocab_processor.vocabulary_))\n",
    "print(\"Messages: %s\" % len(txtTransform))\n",
    "\n",
    "dataNp=np.array(txtTransform)\n",
    "data=dataNp.flat\n",
    "print(\"Data len: %s\" % len(data))\n",
    "\n",
    "##### Eliminar tokens Repetidos en el FLAT\n",
    "data=[k for k, g in itertools.groupby(data)]\n",
    "print(\"Total Tokens: %s\" % len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPmGt6Est0yH"
   },
   "source": [
    "# Divide los mensajes largos en N numeros de lineas y elimina las lineas que sean ceros.\n",
    "\n",
    "dataNp=np.array(txtTransform)\n",
    "print(\"Messages full: %s\" % len(dataNp))\n",
    "\n",
    "x=dataNp\n",
    "b=[]\n",
    "for i in xrange(len(x)):\n",
    "    a=np.split(x[i],5,0) # <<<<-------- Dividir txtLen en cuantas lineas??\n",
    "    for aa in a: \n",
    "        b.append(aa)\n",
    "x=np.array(b)\n",
    "del(b)\n",
    "print(\"Messages post split: %s\" % len(x))\n",
    "\n",
    "delRow=np.nonzero(x.sum(axis=1) == 0)\n",
    "x = np.delete(x, delRow, axis=0)\n",
    "\n",
    "print(\"Messages post delete zeros: %s\" % len(x))\n",
    "\n",
    "\n",
    "dataNp=x\n",
    "del(x)\n",
    "\n",
    "data=dataNp.flat\n",
    "\n",
    "print(dataNp[:10])\n",
    "print(\"\\n\")\n",
    "print(data[:100])\n",
    "print(\"Total Tokens: %s\" % len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ghYDE1it0yI"
   },
   "outputs": [],
   "source": [
    "data_index=0\n",
    "i_message=0\n",
    "\n",
    "hist, bins = np.histogram(data,range=(0,50), bins=50,density=False)\n",
    "width = 0.7 * (bins[1] - bins[0])\n",
    "center = (bins[:-1] + bins[1:]) / 2\n",
    "plt.bar(center, hist, align='center', width=width)\n",
    "plt.show()\n",
    "print([float(hi)/len(data) for hi in hist[:10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCFm0NFct0yK"
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    global i_message\n",
    "    global txtLen\n",
    "    global messages_batch\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    i_message+=1\n",
    "    ##aux_data_index=(txtLen*messages_batch*i_message)-(skip_window*2)\n",
    "    aux_data_index=(txtLen*i_message)-(skip_window*2)\n",
    "    if((aux_data_index>len(data)) or ((aux_data_index+batch_size)>len(data))):\n",
    "        data_index = (data_index + len(data) - span) % len(data)\n",
    "    else:\n",
    "        data_index=aux_data_index\n",
    "    return batch, labels\n",
    "\n",
    "#messages_batch=1\n",
    "\n",
    "num_skips=2\n",
    "skip_window=1\n",
    "#batch_size=100\n",
    "#batch_size=(messages_batch*txtLen*num_skips)-(num_skips*skip_window*2)\n",
    "#batch_size=(messages_batch*txtLen*num_skips)-(skip_window*2)\n",
    "#batch_size=(messages_batch*txtLen*num_skips)+(skip_window)\n",
    "#batch_size=(messages_batch*txtLen*num_skips)\n",
    "batch_size=(txtLen*num_skips)\n",
    "batch, labels = generate_batch(batch_size, num_skips, skip_window)# 16 a 512, 8,2,1\n",
    "\n",
    "print(\"batch: \",len(batch))\n",
    "print(\"labels: \",len(labels))\n",
    "print(\"data-index\",data_index)\n",
    "#for i in range(batch_size):\n",
    "#    print(i,batch[i], vocab_processor.vocabulary_.reverse(batch[i]),\n",
    "#        '->', labels[i, 0], vocab_processor.vocabulary_.reverse(labels[i, 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlIcV71tt0yM"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vocabulary_size=len(vocab_processor.vocabulary_)\n",
    "print(\"Vocabulary: %s \" % vocabulary_size)\n",
    "#messages_batch=1\n",
    "embedding_size = 128  # Dim embedding vector.\n",
    "skip_window = 2       # How many words left and right. \n",
    "num_skips = 2         # How many times to reuse an input to generate a label. Para dar un poco de euristica\n",
    "#batch_size = 128  # 128\n",
    "#batch_size=(messages_batch*txtLen*num_skips)\n",
    "batch_size=(txtLen*num_skips)\n",
    "print(\"Batch size: %s\" % batch_size)\n",
    "\n",
    "# Random validation set to sample nearest neighbors.\n",
    "# limit the validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 10     # Random set of words to evaluate similarity on.\n",
    "valid_window = 500  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size],name=\"train_input\")\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1],name=\"train_labels\")\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32,name=\"valid_dataset\")\n",
    "\n",
    "    with tf.device('/cpu:0'),tf.name_scope(\"embeds\"):\n",
    "\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name=\"embeddings\")\n",
    "        #variable_summaries(embeddings)\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs,name=\"lookup\")\n",
    "\n",
    "    with tf.device('/cpu:0'),tf.name_scope(\"nce-loss\"):\n",
    "        # NCE loss, noise-contrastive estimation loss , a logistic regression model\n",
    "        nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                                      stddev=1.0 / math.sqrt(embedding_size)),\n",
    "                                  name=\"nce_Ws\")\n",
    "        nce_biases = tf.Variable(tf.zeros([vocabulary_size]),name=\"nce_Bs\")\n",
    "\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                             biases=nce_biases,\n",
    "                                             labels=train_labels,\n",
    "                                             inputs=embed,\n",
    "                                             num_sampled=num_sampled,\n",
    "                                             num_classes=vocabulary_size))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    #optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "    with tf.device('/cpu:0'),tf.name_scope(\"cosine-sim\"):\n",
    "        # cosine similarity between minibatch examples and all embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "        similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    \n",
    "    \n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whz4qZp1t0yO"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_steps = 150001 # 1M\n",
    "embed_viz_size = 2000 #10K\n",
    "data_index=0\n",
    "i_message=0\n",
    "timestamp = str(int(time.time()))\n",
    "\n",
    "print(\"Lanzamiento No: \", timestamp)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(\"embed/\" + timestamp + '/train',\n",
    "                                          session.graph)\n",
    "    init.run()\n",
    "    \n",
    "    if(1):   # <<<<<<<<<<<<<<--------------------- Restore previous embedds\n",
    "        saverR = tf.train.Saver({\"embeddingsTot\": embeddings})\n",
    "        saverR.restore(session, \"embed/\"+\"1491808882\"+\"/train/modelskip.ckpt-1\")\n",
    "        print(\"EMBEDS RESTORED  <<<<<<<<<<<<\")\n",
    "    print(\"Initialized\")\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "        summary,_, loss_val = session.run([merged,optimizer, loss], feed_dict=feed_dict)\n",
    "        train_writer.add_summary(summary, step)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        if step % 1500 == 0:    # <<<<<<<-------------calculate average loss\n",
    "            if step > 0:\n",
    "                average_loss /= 1500\n",
    "            print(\"Average loss at step \", step, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "        if (step % 15000 == 0): # 100K   <<<<<<<<<----------- Similarity\n",
    "#        if (step % 10000 == 0) and (False):\n",
    "            sim = similarity.eval()\n",
    "            for i in xrange(valid_size):\n",
    "                valid_word = vocab_processor.vocabulary_.reverse(valid_examples[i])\n",
    "                top_k = 5  #  <<<<---------------    number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = \"Nearest to %s:\" % valid_word\n",
    "                for k in xrange(top_k):\n",
    "                    close_word = vocab_processor.vocabulary_.reverse(nearest[k])\n",
    "                    log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                print(log_str)\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    \n",
    "    # For embedd visualization\n",
    "    print(\"\\n#### Skip-gram end\")\n",
    "    tsvLabels = [re.sub(r\"\\n\", \"\",vocab_processor.vocabulary_.reverse(i)).encode('utf8')+\"\\n\" for i in xrange(embed_viz_size)]\n",
    "    fLabels=open(\"embed/\"+timestamp+\"/train/metadata.tsv\",\"w\")\n",
    "    print(\"tsvLabels: \")\n",
    "    print(len(tsvLabels))\n",
    "    print(type(tsvLabels))\n",
    "    fLabels.writelines(tsvLabels)\n",
    "    fLabels.close()\n",
    "    \n",
    "    embedding_var = tf.Variable(normalized_embeddings[:embed_viz_size], name='embedding_labels')\n",
    "    session.run(embedding_var.initializer)\n",
    "    embeddingTotNorm = tf.Variable(normalized_embeddings, name='embeddingsTotNorm')\n",
    "    session.run(embeddingTotNorm.initializer)\n",
    "    embeddingTot = tf.Variable(embeddings, name='embeddingsTot')\n",
    "    session.run(embeddingTot.initializer)\n",
    "    \n",
    "    # Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n",
    "    config = projector.ProjectorConfig()\n",
    "    # Use the same LOG_DIR where you stored your checkpoint.\n",
    "    summary_writer = tf.summary.FileWriter(\"embed/\"+timestamp+\"/train\")\n",
    "    # You can add multiple embeddings. Here we add only one.\n",
    "    embeddingConf = config.embeddings.add()\n",
    "    embeddingConf.tensor_name = embedding_var.name\n",
    "    embeddingConf.metadata_path = os.path.join(\"embed/\"+timestamp+\"/train/\", 'metadata.tsv')\n",
    "    # Saves a configuration file that TensorBoard will read during startup.\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "    # save checkpoint\n",
    "    saver = tf.train.Saver([embedding_var,embeddingTotNorm,embeddingTot])\n",
    "    saver.save(session, os.path.join(\"embed/\"+timestamp+\"/train/\", \"modelskip.ckpt\"), 1)\n",
    "    \n",
    "    # Vocabulary\n",
    "    vocab_processor.save(\"embed/\"+timestamp+\"/train/vocabulary.txt\")\n",
    "    print(\"\\n#### embedds saved\")\n",
    "    \n",
    "    \n",
    "print(\"index: \",data_index)\n",
    "print(\"data len: \",len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67bp7wm0t0yR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dqOGy_01t0yT"
   },
   "outputs": [],
   "source": [
    "#!rm -r embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TlslBw1St0yV"
   },
   "source": [
    "# Recuperar embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1d6bCUT4t0yW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "modelN=\"1490055956\"\n",
    "timestamp=modelN\n",
    "vocabulary_size=2757\n",
    "embedding_size=128\n",
    "embeddingsTotNorm = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name=\"embeddingsTotNorm\")\n",
    "saver = tf.train.Saver({\"embeddingsTotNorm\": embeddingsTotNorm})\n",
    "#saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"embed/\"+timestamp+\"/train/modelskip.ckpt-1\")\n",
    "    embedNp=sess.run(embeddingsTotNorm)\n",
    "    print(\"Model restored.\")\n",
    "    print(\"Embeds: %s\" % len(embedNp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjl7Wu6rt0yY"
   },
   "outputs": [],
   "source": [
    "NLTK_TWEET_TOKENIZER = TweetTokenizer(preserve_case=False,reduce_len=True)\n",
    "\n",
    "def tokenizerFn(iterator):\n",
    "    for value in iterator:\n",
    "        try:\n",
    "            yield NLTK_TWEET_TOKENIZER.tokenize(value)\n",
    "        except TypeError:       # this is a hack to avoid the error\n",
    "            yield []     \n",
    "\n",
    "txtLen=50\n",
    "\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(txtLen,min_frequency=1,tokenizer_fn=tokenizerFn)\n",
    "vr=vocab_processor.restore(\"embed/\"+timestamp+\"/train/vocabulary.txt\")\n",
    "len(vr.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "akfoaLNst0ya"
   },
   "outputs": [],
   "source": [
    "print(vr.vocabulary_.reverse(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HrRSHkMFt0yd"
   },
   "source": [
    "# Empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RG81VGwSt0yd"
   },
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (15.0, 10.0)\n",
    "\n",
    "\n",
    "lexicon = Empath()\n",
    "print(len(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wdo0pjfCt0yg"
   },
   "outputs": [],
   "source": [
    "NLTK_TWEET_TOKENIZER_EMPATH = TweetTokenizer(preserve_case=False,reduce_len=True)\n",
    "\n",
    "def tokenizerFnDoc(doc):\n",
    "    for tk in NLTK_TWEET_TOKENIZER_EMPATH.tokenize(doc):\n",
    "        try:\n",
    "            yield tk\n",
    "        except TypeError:       # this is a hack to avoid the error\n",
    "            yield []  \n",
    "\n",
    "def clean_chats(string):\n",
    "    string=re.sub(r\"\\S{15,}\\:\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\s\\:\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\,\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\s\\,\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\-\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{15,}\\s\\-\\s\",\"\",string)\n",
    "    string=re.sub(r\"\\S{20,}\",\"\",string)\n",
    "    return string.strip()\n",
    "def dummy(string):\n",
    "    string=clean_chats(string)\n",
    "    return string\n",
    "\n",
    "\n",
    "lines = open('PAN12/atacantes.txt').readlines()\n",
    "lines=[unicode(w,errors='ignore') for w in lines if len(w)<500]\n",
    "\n",
    "numthreads = 12\n",
    "numlines = 1000\n",
    "result_list=[]\n",
    "\n",
    "pool = multiprocessing.Pool(processes=numthreads)\n",
    "result_list = pool.map_async(dummy, lines, numlines)\n",
    "\n",
    "txt=result_list.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3rQLjh4Dt0yl"
   },
   "outputs": [],
   "source": [
    "#lex=lexicon.analyze(txt,tokenizer=tokenizerFnDoc,normalize=True)\n",
    "lex=lexicon.analyze(txt,tokenizer=tokenizerFnDoc,normalize=False)\n",
    "#lex=lexicon.analyze(txt,normalize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8jXuI1et0yo"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "########\n",
    "########\n",
    "##  ORDEN POR VALOR\n",
    "\n",
    "lexLstSort =sorted(lex.items(), key=operator.itemgetter(1))\n",
    "\n",
    "lexLstSort=list(reversed(lexLstSort))\n",
    "lexv=[a[1] for a in lexLstSort[:30] if (a[1]>0)]\n",
    "lexl=[a[0] for a in lexLstSort[:30] if (a[1]>0)]\n",
    "\n",
    "print(lexl[:5])\n",
    "print(lexv[:5])\n",
    "\n",
    "alphab = lexl\n",
    "frequencies = lexv\n",
    "\n",
    "pos = np.arange(len(alphab))\n",
    "width = 1.0     # gives histogram aspect to the bar diagram\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(pos + (width / 2))\n",
    "ax.set_xticklabels(alphab)\n",
    "plt.xticks( rotation='vertical')\n",
    "plt.bar(pos, frequencies, width)\n",
    "plt.show()\n",
    "\n",
    "######\n",
    "######\n",
    "##   ORDEN EMPATH\n",
    "\n",
    "lexv=[a[1] for a in lex.items() if (a[1]>0)]\n",
    "lexl=[a[0] for a in lex.items() if (a[1]>0)]\n",
    "\n",
    "alphab = lexl\n",
    "frequencies = lexv\n",
    "\n",
    "pos = np.arange(len(alphab))\n",
    "width = 1.0     # gives histogram aspect to the bar diagram\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(pos + (width / 2))\n",
    "ax.set_xticklabels(alphab)\n",
    "plt.xticks( rotation='vertical')\n",
    "plt.bar(pos, frequencies, width)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxbwzhNRt0ys"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "#print(embedNp[:2])\n",
    "res=pairwise_distances(embedNp, metric='cosine',n_jobs=-2)\n",
    "print(len(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcAIx5zwt0yv"
   },
   "outputs": [],
   "source": [
    "lexCats=lexicon.cats\n",
    "#lexCatsLst=[item[0] for item in lexicon.cats.items()]\n",
    "#lexItemsLst=[item[1] for item in lexicon.cats.items()]\n",
    "\n",
    "\n",
    "vocListNp=np.array([vr.vocabulary_.reverse(i) for i in xrange(len(vr.vocabulary_))])\n",
    "print(\"voc: %s\" % len(vocList))\n",
    "network={}\n",
    "for cat in lexLstSort[:10]:\n",
    "    itemCatNp=np.array(lexCats[cat[0]])\n",
    "    intersec=np.intersect1d(vocListNp,itemCatNp)\n",
    "    b=[]\n",
    "    for term in intersec:\n",
    "        i=vr.vocabulary_.get(term)\n",
    "        row=res[i]\n",
    "        row[i]=1\n",
    "        ii=np.argmin(row)\n",
    "        b.append(vr.vocabulary_.reverse(ii))\n",
    "    intersecAux=list(intersec)+b\n",
    "    intersecAux=list(set(intersecAux))\n",
    "    network[cat[0]]=intersecAux\n",
    "len(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DuLBn4ot0yx"
   },
   "outputs": [],
   "source": [
    "class User(object):\n",
    "    def __init__(self, s, t):\n",
    "        self.source = s\n",
    "        self.target = t\n",
    "networkLst=[]\n",
    "nodes=[]\n",
    "for a in network:\n",
    "    nodes.append(a)\n",
    "    for ai in network[a]:\n",
    "        networkLst.append(User(a,ai))\n",
    "        nodes.append(ai)\n",
    "#print(nodes)\n",
    "\n",
    "#nodes=list(set(nodes))\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def jdefault(o):\n",
    "    return o.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9oJwULMt0yz"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from string import Template\n",
    "import pandas as pd\n",
    "import json, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "flT8qCant0y2"
   },
   "outputs": [],
   "source": [
    "HTML('''\n",
    "<script src=\"lib/sigmajs/sigma.min.js\"></script>\n",
    "<script src=\"js/sigma-add-method-neighbors.js\"></script>\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o5tqTzdtt0y6"
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "graph_data = { 'nodes': [], 'edges': [] }\n",
    "\n",
    "nodesU=[]\n",
    "for i in nodes:\n",
    "    if(type(i)==np.unicode_):\n",
    "        dd=i.encode('utf8')\n",
    "        dd=str(dd)\n",
    "    else:\n",
    "        dd=str(i)\n",
    "    nodesU.append(dd)\n",
    "    \n",
    "print(len(nodesU))\n",
    "nodesU=list(set(nodesU))\n",
    "print(len(nodesU))\n",
    "print(len(networkLst))\n",
    "\n",
    "\n",
    "for i in nodesU:\n",
    "    \n",
    "    siz=lex.get(i)\n",
    "    if siz is None:\n",
    "        siz=0.3\n",
    "    else:\n",
    "        siz=siz/750 #1000\n",
    "    graph_data['nodes'].append({\n",
    "            \"id\": str(i),\n",
    "            \"label\": str(i),\n",
    "            \"x\": random.uniform(0,1),\n",
    "            \"y\": random.uniform(0,1),\n",
    "            \"size\": siz\n",
    "        })\n",
    "\n",
    "ii=0\n",
    "for j in networkLst:\n",
    "    x_center = random.uniform(0,1)\n",
    "    y_center = random.uniform(0,1)\n",
    "    x_dist = random.uniform(0.1,0.5)\n",
    "    y_dist = random.uniform(0.2,0.5)\n",
    "    graph_data['edges'].append({\n",
    "            \"id\": \"e\" + str(ii),\n",
    "            \"source\": str(j.source),\n",
    "            \"target\": str(j.target)\n",
    "        })\n",
    "    ii+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZ5nQsKot0y7"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(graph_data['nodes']).head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJ78TDKvt0y-"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(graph_data['edges']).head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vNsTc9j-t0zD"
   },
   "outputs": [],
   "source": [
    "js_text_template = Template(open('js/sigma-graph.js','r').read())\n",
    "\n",
    "js_text = js_text_template.substitute({'graph_data': json.dumps(graph_data),\n",
    "                                       'container': 'graph-div'})\n",
    "\n",
    "html_template = Template('''\n",
    "<div id=\"graph-div\" style=\"width: 900px;max-width: 900px;height:800px\"></div>\n",
    "<script> $js_text </script>\n",
    "''')\n",
    "HTML(html_template.substitute({'js_text': js_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IF_yCTvJt0zF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copia de IntegracionCorpus-checkpoint.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
